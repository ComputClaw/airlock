# Airlock Architecture

## Core Principle: Deterministic Execution

> **Same code + same source data = same numbers. Every time.**

This is the foundational rule of Airlock. All data fetching, aggregation, calculation, and report building runs as deterministic Python. The LLM is optional and strictly for presentation — summaries, conclusions, narrative insights. The data itself is never generated by an LLM.

Why this matters:
- Reports are auditable and reproducible
- Bugs are debuggable (re-run the same code, get the same result)
- LLM non-determinism is confined to a clearly separated layer
- Trust: stakeholders can verify numbers independently

---

## Overview

Airlock is a Python code execution service that sits between AI agents and authenticated infrastructure. It accepts code from agents, runs it in isolated worker containers with injected secrets, and returns sanitized results via a polling API.

```
┌──────────────────────────────────────────────────────────────────┐
│                         AI Agent                                 │
│                                                                  │
│  1. POST /execute {code, project}         → 202 {execution_id}  │
│  2. GET  /executions/{id}                 → {status: "running"}  │
│  3. GET  /executions/{id}                 → {status: "awaiting_  │
│                                               llm", prompt: ...} │
│  4. POST /executions/{id}/respond {resp}  → 200                  │
│  5. GET  /executions/{id}                 → {status: "completed",│
│                                               result: ...}       │
└──────────────────┬───────────────────────────────────────────────┘
                   │ HTTPS
┌──────────────────▼───────────────────────────────────────────────┐
│                     Airlock API Server                            │
│                     (FastAPI — router/manager)                    │
│                                                                  │
│  • Accepts execution requests                                    │
│  • Routes to idle worker in the project's pool                   │
│  • Tracks execution state (pending/running/awaiting_llm/...)     │
│  • Returns sanitized results                                     │
└────────┬─────────────────┬─────────────────┬─────────────────────┘
         │                 │                 │
    ┌────▼────┐       ┌────▼────┐       ┌────▼────┐
    │ Worker  │       │ Worker  │       │ Worker  │    Project: "reports"
    │  :8001  │       │  :8001  │       │  :8001  │    (3 replicas)
    │ FastAPI │       │ FastAPI │       │ FastAPI │
    │ /run    │       │ /run    │       │ /run    │
    └─────────┘       └─────────┘       └─────────┘

    ┌─────────┐       ┌─────────┐
    │ Worker  │       │ Worker  │                      Project: "analytics"
    │  :8001  │       │  :8001  │                      (2 replicas)
    │ FastAPI │       │ FastAPI │
    │ /run    │       │ /run    │
    └─────────┘       └─────────┘
```

---

## Worker Pool Model

### Why Not Container-per-Execution?

Spinning up a fresh container for every code execution adds 1-3 seconds of cold-start overhead, requires rebuilding the environment each time, and wastes resources. A warm pool eliminates all of this.

### How It Works

1. **Project Create**: User defines a project — packages, secrets, network rules
2. **Image Build**: Airlock builds a Docker image with pre-installed packages
3. **Pool Up**: `project up --replicas N` spins up N long-running worker containers
4. **Request Routing**: Airlock API routes incoming `/execute` requests to an idle worker
5. **Execution**: Worker runs code in a thread, returns results via HTTP
6. **Pool Down**: `project down` tears down all workers for the project

### Worker Internals

Each worker container runs a minimal FastAPI server:

```
Worker Container
├── FastAPI server on :8001
│   └── POST /run
│       ├── Receives {code, settings, memory}
│       ├── Executes code in a thread
│       ├── If llm.complete() called → returns {status: "awaiting_llm", ...}
│       ├── On resume → continues execution
│       └── Returns {status, result, stdout, stderr, memory_updates}
├── ENV: secrets injected at container start
├── SDK: settings, memory, llm, set_result available to scripts
└── Runs as non-root user
```

### Pool Lifecycle

```
project create "reports"
  → defines packages, secrets, network rules

project up "reports" --replicas 3
  → builds image (if needed)
  → starts 3 worker containers
  → registers them in the routing table

POST /execute {project: "reports", code: "..."}
  → finds idle worker → forwards to worker /run
  → tracks execution state

project down "reports"
  → stops and removes all worker containers
  → clears routing table
```

---

## Polling-Based API

### Why Polling (Not Callbacks)?

- **No open ports needed** on the agent side
- **Works behind NAT**, firewalls, anywhere
- **No webhook infrastructure** to maintain
- **Simple**: agent just polls until done
- **LLM integration** fits naturally — `awaiting_llm` is just another status

### Execution Statuses

| Status | Meaning |
|--------|---------|
| `pending` | Queued, waiting for idle worker |
| `running` | Code is executing in a worker |
| `awaiting_llm` | Script called `llm.complete()`, waiting for agent to provide LLM response |
| `completed` | Finished successfully |
| `error` | Execution failed (exception, invalid code, etc.) |
| `timeout` | Exceeded time limit |

### API Endpoints

#### `POST /execute`

Submit code for execution. Returns immediately with an execution ID.

```
Request:
{
  "project": "reports",
  "code": "import requests\ndata = requests.get(settings.get('API_URL'))...",
  "timeout": 60,
  "settings": {"REPORT_TYPE": "weekly"},    // non-secret settings
  "memory": {"last_run": "2025-01-15"}      // current memory state
}

Response: 202 Accepted
{
  "execution_id": "exec_a1b2c3d4",
  "status": "pending"
}
```

#### `GET /executions/{id}`

Poll for execution status and results.

```
Response (running):
{
  "execution_id": "exec_a1b2c3d4",
  "status": "running"
}

Response (awaiting LLM):
{
  "execution_id": "exec_a1b2c3d4",
  "status": "awaiting_llm",
  "llm_request": {
    "prompt": "Summarize this revenue data: ...",
    "model": "claude-sonnet"
  }
}

Response (completed):
{
  "execution_id": "exec_a1b2c3d4",
  "status": "completed",
  "result": { ... },              // whatever set_result() was called with
  "stdout": "...",                // sanitized
  "stderr": "...",                // sanitized
  "memory_updates": {             // changes to apply to memory
    "last_run": "2025-02-01"
  },
  "execution_time_ms": 1234
}

Response (error):
{
  "execution_id": "exec_a1b2c3d4",
  "status": "error",
  "error": "NameError: name 'foo' is not defined",
  "stdout": "...",
  "stderr": "...",
  "execution_time_ms": 89
}
```

#### `POST /executions/{id}/respond`

Provide an LLM response to a paused execution.

```
Request:
{
  "response": "Based on the revenue data, Q4 showed a 15% increase..."
}

Response: 200 OK
{
  "execution_id": "exec_a1b2c3d4",
  "status": "running"
}
```

#### `GET /projects`

List available projects and their status.

```
Response:
{
  "projects": [
    {
      "name": "reports",
      "description": "Oracle Simphony revenue reporting",
      "status": "up",
      "replicas": 3,
      "idle_workers": 2,
      "packages": ["requests", "pandas", "openpyxl"]
    }
  ]
}
```

#### `POST /projects/{name}/up`

Start the worker pool for a project.

```
Request:
{
  "replicas": 3
}

Response: 200 OK
{
  "name": "reports",
  "status": "up",
  "replicas": 3
}
```

#### `POST /projects/{name}/down`

Stop and remove all workers for a project.

```
Response: 200 OK
{
  "name": "reports",
  "status": "down",
  "replicas": 0
}
```

#### `GET /health`

```
Response: 200 OK
{
  "status": "ok"
}
```

---

## LLM Integration via Pause/Resume

### Design Principle

Airlock holds **zero** LLM credentials. It is purely an execution service. The AI/non-deterministic part stays entirely on the agent side.

### How It Works

```
┌─────────┐         ┌──────────┐         ┌──────────┐
│  Agent   │         │ Airlock  │         │  Worker  │
│          │         │   API    │         │          │
│ POST     │────────▶│          │────────▶│ run code │
│ /execute │         │          │         │          │
│          │         │          │         │ ...      │
│          │         │          │◀────────│ llm.     │
│          │         │          │ pause   │ complete()│
│          │         │  status: │         │          │
│ GET      │────────▶│  await_  │         │ (blocked)│
│ /exec/id │◀────────│  llm     │         │          │
│          │         │          │         │          │
│ run LLM  │         │          │         │          │
│ locally  │         │          │         │          │
│          │         │          │         │          │
│ POST     │────────▶│          │────────▶│ resume   │
│ /respond │         │          │         │          │
│          │         │          │◀────────│ done     │
│ GET      │────────▶│ status:  │         │          │
│ /exec/id │◀────────│ completed│         │          │
└─────────┘         └──────────┘         └──────────┘
```

1. Script calls `llm.complete(prompt, model)`
2. Worker pauses execution, returns `{status: "awaiting_llm", prompt: "...", model: "..."}`
3. Airlock API stores the LLM request and updates execution status
4. Agent polls, sees `awaiting_llm`, reads the prompt
5. Agent runs the prompt through its own LLM (agent-side, with agent's own credentials)
6. Agent POSTs the response to `/executions/{id}/respond`
7. Airlock forwards the response to the worker
8. Worker resumes execution with the LLM response
9. Script continues — e.g., formatting the response into a report

### Why This Pattern?

- **No API keys in Airlock**: The execution environment never needs LLM credentials
- **Agent controls the model**: Agent can use whatever model, provider, or parameters it wants
- **Deterministic data + LLM presentation**: The script computes all the numbers deterministically, then optionally asks the LLM to write a narrative around them
- **Auditable**: The exact LLM prompt and response are visible in the execution record

---

## Script SDK

These functions are available to scripts running inside workers:

### `settings.get(key) → str`

Get a setting value. Secrets are resolved from environment variables (injected at container start). Non-secret settings are passed in the execution payload.

### `settings.keys() → list[str]`

List all available setting keys.

### `memory.get(category, key) → any`

Read a value from persistent memory. Memory is passed in with the execution request.

### `memory.set(category, key, value)`

Write a value to persistent memory. Changes are collected and returned in the execution response as `memory_updates`. They are not applied until the execution completes successfully.

### `llm.complete(prompt, model="default") → str`

Pause execution and request an LLM completion from the agent. The execution status changes to `awaiting_llm`. When the agent provides a response via `/executions/{id}/respond`, execution resumes and this function returns the LLM's response.

**Use sparingly.** Only for presentation — summaries, narratives, insights. Never for data computation.

### `set_result(data)`

Set the structured return value of the script. `data` can be any JSON-serializable value. This is what appears in the `result` field of the completed execution response.

---

## Settings vs Memory

| | Settings | Memory |
|---|----------|--------|
| **What** | User-provided configuration | Agent-learned state |
| **Examples** | API keys, base URLs, preferences | Last run timestamp, counters, cache |
| **Who writes** | User/admin configures | Scripts read and write |
| **Script access** | Read-only (`settings.get`) | Read-write (`memory.get`, `memory.set`) |
| **Conflict rule** | **Settings wins** | Memory yields |
| **Where stored** | Project config + env vars | Execution payload + response |

### How Settings Work

- **Secrets** (API keys, tokens): Defined in project config, injected as environment variables at container start. `settings.get("API_KEY")` reads from `os.environ`.
- **Non-secrets** (preferences, config): Passed in the execution request payload. `settings.get("REPORT_TYPE")` reads from the payload.

### How Memory Works

- Agent passes current memory state in the execution request
- Script reads with `memory.get(category, key)`
- Script writes with `memory.set(category, key, value)`
- Changes are **not applied during execution** — they're returned in the response as `memory_updates`
- The agent applies the updates to its memory store after receiving results

---

## Security Model

### Layer 1: Secret Isolation

- Secrets injected as **environment variables** at container start (process-scoped)
- Code sent via HTTP request body — never written to disk
- Scripts access secrets through `settings.get()`, never see raw env var names
- Agent never receives secret values — only project names and setting keys

### Layer 2: Output Sanitization

All output is scanned before being returned to the agent:
- Exact-match against known secret values
- Common patterns: API keys, bearer tokens, connection strings
- Matches replaced with `[REDACTED...last4]` (last 4 chars preserved for debugging)
- Applied to: stdout, stderr, result data, error messages

### Layer 3: Network Isolation

- Each project declares an **allowlist** of hosts the code can reach
- Workers are on a Docker network with iptables rules enforcing the allowlist
- DNS resolution only for allowlisted hosts
- No "phone home" — code can't exfiltrate data to arbitrary endpoints

### Layer 4: Container Isolation

- Per-project worker pools — projects cannot access each other's workers
- Workers run as **non-root** user
- Resource limits: memory cap, CPU cap, execution timeout
- Read-only filesystem (except /tmp)
- `--no-new-privileges` security option

### Layer 5: Worker Pool Isolation

- Each project gets its own set of containers
- Separate Docker networks per project
- Containers see only their project's secrets
- No shared state between projects

### Security Boundary Summary

```
┌─────────────────────────────────────────────────────┐
│                    Agent                             │
│  • Never sees secrets                               │
│  • Receives only sanitized output                   │
│  • Owns LLM credentials (not Airlock)               │
├─────────────────────────────────────────────────────┤
│                  Airlock API                         │
│  • Routes requests to workers                       │
│  • Manages execution lifecycle                      │
│  • Sanitizes all output before return               │
│  • Holds project configs + secret references        │
├─────────────────────────────────────────────────────┤
│              Worker Pool (per project)               │
│  • Secrets as env vars (process-scoped)             │
│  • Allowlisted network only                         │
│  • Non-root, resource-limited                       │
│  • Isolated from other projects' workers            │
└─────────────────────────────────────────────────────┘
```

---

## Data Flow: Complete Execution

```
Agent                    Airlock API               Worker Pool            External API
  │                          │                         │                       │
  │ POST /execute            │                         │                       │
  │ {project, code,          │                         │                       │
  │  settings, memory}       │                         │                       │
  │─────────────────────────▶│                         │                       │
  │                          │ find idle worker         │                       │
  │                          │────────────────────────▶│                       │
  │◀─────────────────────────│                         │                       │
  │ 202 {execution_id}      │ POST /run                │                       │
  │                          │ {code, settings, memory} │                       │
  │                          │────────────────────────▶│                       │
  │                          │                         │ requests.get(API_URL)  │
  │                          │                         │──────────────────────▶│
  │                          │                         │◀──────────────────────│
  │                          │                         │ data                   │
  │ GET /executions/{id}     │                         │                       │
  │─────────────────────────▶│                         │ llm.complete(prompt)   │
  │◀─────────────────────────│◀────────────────────────│ (paused)              │
  │ {status: awaiting_llm,   │                         │                       │
  │  prompt: "..."}          │                         │                       │
  │                          │                         │                       │
  │ [agent runs LLM]         │                         │                       │
  │                          │                         │                       │
  │ POST /executions/{id}/   │                         │                       │
  │   respond {response}     │                         │                       │
  │─────────────────────────▶│────────────────────────▶│ (resumed)             │
  │                          │                         │                       │
  │                          │                         │ set_result(report)     │
  │                          │◀────────────────────────│ done                  │
  │                          │ sanitize output          │                       │
  │ GET /executions/{id}     │                         │                       │
  │─────────────────────────▶│                         │                       │
  │◀─────────────────────────│                         │                       │
  │ {status: completed,      │                         │                       │
  │  result: report,         │                         │                       │
  │  memory_updates: {...}}  │                         │                       │
  │                          │                         │                       │
```

---

## Project Configuration

```yaml
# projects/simphony-reports.yaml
name: simphony-reports
description: "Oracle Simphony revenue and sales reporting"

secrets:
  SIMPHONY_API_KEY: "${vault:simphony_api_key}"
  SIMPHONY_BASE_URL: "https://api.simphony.oracle.com"
  OPERA_API_KEY: "${vault:opera_api_key}"

network_allowlist:
  - "api.simphony.oracle.com"
  - "api.opera.oracle.com"

limits:
  timeout: 60
  memory_mb: 512
  max_output_mb: 10

packages:
  - requests
  - pandas
  - openpyxl
```

---

## Future Considerations

- **Multi-language workers**: Node.js, SQL execution
- **Code caching**: Hash code, skip re-execution for identical requests
- **Audit log**: Every execution logged with code, results, timing
- **OpenClaw skill**: A SKILL.md that teaches any agent how to use Airlock
- **Worker auto-scaling**: Scale replicas based on queue depth
- **Health checks**: Periodic liveness probes on workers, auto-restart unhealthy ones
