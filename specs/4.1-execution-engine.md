# Spec 4.1: Execution Engine (Phase 4 → v0.4.0)

## Goal

Wire real Docker-based execution into the API. Persist execution records in SQLite. Return `poll_url` for load-balancer-safe polling. Add execution history endpoints. This is the phase where Airlock becomes usable — an agent can submit code, get results, and query history.

**Before:** `POST /execute` runs scripts in-memory via `exec()`. Results stored in a Python dict (lost on restart). No execution history API. WorkerManager exists but isn't connected to the execute endpoint.
**After:** `POST /execute` routes scripts through the Docker worker container. Execution records persisted in SQLite. `poll_url` returned in response. Execution history queryable via admin and agent APIs. Credentials injected into the worker via the profile system.

## Prerequisites

- Spec 1.1 (Foundation) ✅
- Spec 1.2 (Docker Execution) ✅ — WorkerManager + worker container exist
- Spec 2.1 (Credential Management) ✅ — `resolve_profile_credentials` ready
- Spec 3.1 (Profile System) ✅ — Bearer auth + HMAC, profile lifecycle

---

## Architecture

```
Agent                           Airlock API                     Docker Worker
  │                                │                                │
  │ POST /execute                  │                                │
  │ Authorization: Bearer ark_...  │                                │
  │ {script, hash, timeout}        │                                │
  │───────────────────────────────▶│                                │
  │                                │  1. Auth profile (require_profile)
  │                                │  2. Verify HMAC                │
  │                                │  3. Resolve credentials        │
  │                                │  4. Insert execution (SQLite)  │
  │◀───────────────────────────────│                                │
  │ 202 {execution_id, poll_url,   │                                │
  │      status: "pending"}        │                                │
  │                                │  5. Send to worker             │
  │                                │───────────────────────────────▶│
  │                                │                                │ exec(script)
  │                                │                                │ settings.get()
  │                                │◀───────────────────────────────│
  │                                │  6. Update execution (SQLite)  │
  │                                │     status, result, timing     │
  │                                │                                │
  │ GET /executions/{id}           │                                │
  │───────────────────────────────▶│                                │
  │◀───────────────────────────────│                                │
  │ {status: "completed",          │                                │
  │  result: {...}, stdout: "..."} │                                │
```

---

## Tasks

### 1. Execution Service — `src/airlock/services/executions.py` (new file)

Business logic for execution management. Encapsulates SQLite persistence and worker dispatch.

```python
"""Execution management: create, dispatch, poll, and list executions."""

import json
import time
import uuid
from typing import Any, TypedDict

import aiosqlite

class ExecutionRecord(TypedDict):
    """Execution record from SQLite."""
    id: str
    profile_id: str
    status: str              # pending | running | completed | error | timeout
    result: Any | None       # JSON-decoded
    stdout: str
    stderr: str
    error: str | None
    execution_time_ms: int | None
    created_at: str
    completed_at: str | None


async def create_execution(
    db: aiosqlite.Connection,
    profile_id: str,
    script: str,
    timeout: int = 60,
) -> str:
    """Create a new execution record in pending state. Returns execution_id."""
    execution_id = f"exec_{uuid.uuid4().hex[:16]}"
    await db.execute(
        """INSERT INTO executions (id, profile_id, script, status)
           VALUES (?, ?, ?, 'pending')""",
        (execution_id, profile_id, script),
    )
    await db.commit()
    return execution_id


async def update_execution(
    db: aiosqlite.Connection,
    execution_id: str,
    status: str,
    result: Any | None = None,
    stdout: str = "",
    stderr: str = "",
    error: str | None = None,
    execution_time_ms: int | None = None,
) -> None:
    """Update an execution record with results."""
    await db.execute(
        """UPDATE executions
           SET status = ?, result = ?, stdout = ?, stderr = ?,
               error = ?, execution_time_ms = ?,
               completed_at = CASE WHEN ? IN ('completed', 'error', 'timeout')
                              THEN datetime('now') ELSE completed_at END
           WHERE id = ?""",
        (
            status,
            json.dumps(result) if result is not None else None,
            stdout, stderr, error, execution_time_ms,
            status,  # for the CASE expression
            execution_id,
        ),
    )
    await db.commit()


async def get_execution(
    db: aiosqlite.Connection, execution_id: str
) -> ExecutionRecord | None:
    """Get a single execution by ID."""
    cursor = await db.execute(
        """SELECT id, profile_id, status, result, stdout, stderr,
                  error, execution_time_ms, created_at, completed_at
           FROM executions WHERE id = ?""",
        (execution_id,),
    )
    row = await cursor.fetchone()
    if row is None:
        return None

    return ExecutionRecord(
        id=row["id"],
        profile_id=row["profile_id"],
        status=row["status"],
        result=json.loads(row["result"]) if row["result"] else None,
        stdout=row["stdout"] or "",
        stderr=row["stderr"] or "",
        error=row["error"],
        execution_time_ms=row["execution_time_ms"],
        created_at=row["created_at"],
        completed_at=row["completed_at"],
    )


async def list_executions(
    db: aiosqlite.Connection,
    profile_id: str | None = None,
    status: str | None = None,
    limit: int = 50,
    offset: int = 0,
) -> list[ExecutionRecord]:
    """List executions with optional filtering.

    - profile_id: filter by profile
    - status: filter by status
    - limit/offset: pagination (default 50 per page)
    - Ordered by created_at DESC (newest first)
    """
    conditions: list[str] = []
    params: list[str | int] = []

    if profile_id:
        conditions.append("profile_id = ?")
        params.append(profile_id)
    if status:
        conditions.append("status = ?")
        params.append(status)

    where = f"WHERE {' AND '.join(conditions)}" if conditions else ""
    params.extend([limit, offset])

    cursor = await db.execute(
        f"""SELECT id, profile_id, status, result, stdout, stderr,
                   error, execution_time_ms, created_at, completed_at
            FROM executions {where}
            ORDER BY created_at DESC
            LIMIT ? OFFSET ?""",
        params,
    )
    rows = await cursor.fetchall()

    return [
        ExecutionRecord(
            id=row["id"],
            profile_id=row["profile_id"],
            status=row["status"],
            result=json.loads(row["result"]) if row["result"] else None,
            stdout=row["stdout"] or "",
            stderr=row["stderr"] or "",
            error=row["error"],
            execution_time_ms=row["execution_time_ms"],
            created_at=row["created_at"],
            completed_at=row["completed_at"],
        )
        for row in rows
    ]
```

### 2. Update Agent API — `src/airlock/api/agent.py`

Replace the in-memory mock execution with real Docker worker execution + SQLite persistence.

#### Remove

- The `_executions: dict[str, ExecutionResult] = {}` in-memory dict
- The `_run_execution()` background task that uses `exec()`
- The `_worker_manager` global and `set_worker_manager()` function (move worker access to app.state)

#### `POST /execute` → 202

Updated flow:

```python
@router.post("/execute", status_code=202)
async def execute(
    body: ExecutionRequest,
    raw_request: Request,
    background: BackgroundTasks,
    profile: ProfileAuth = Depends(require_profile),
) -> dict:
    """Submit a script for execution."""

    # 1. Verify HMAC
    if not verify_script_hmac(profile.secret, body.script, body.hash):
        raise HTTPException(status_code=403, detail="Script hash verification failed")

    # 2. Resolve credentials
    db = await get_db()
    master_key = raw_request.app.state.master_key
    settings = await resolve_profile_credentials(db, profile.profile_id, master_key)

    # 3. Create execution record in SQLite
    execution_id = await create_execution(db, profile.profile_id, body.script, body.timeout)

    # 4. Build poll_url (full URL for load-balancer safety)
    base_url = str(raw_request.base_url).rstrip("/")
    poll_url = f"{base_url}/executions/{execution_id}"

    # 5. Dispatch to worker in background
    worker: WorkerManager = raw_request.app.state.worker_manager
    background.add_task(
        _dispatch_to_worker, db, worker, execution_id, body.script, settings, body.timeout
    )

    return {
        "execution_id": execution_id,
        "poll_url": poll_url,
        "status": "pending",
    }
```

Background dispatch function:

```python
async def _dispatch_to_worker(
    db: aiosqlite.Connection,
    worker: WorkerManager,
    execution_id: str,
    script: str,
    settings: dict[str, str],
    timeout: int,
) -> None:
    """Send script to Docker worker and persist results."""
    import time

    # Update status to running
    await update_execution(db, execution_id, status="running")

    start = time.monotonic()
    try:
        result = await worker.execute(script, settings, timeout)
        elapsed_ms = int((time.monotonic() - start) * 1000)

        await update_execution(
            db, execution_id,
            status=result["status"],
            result=result.get("result"),
            stdout=result.get("stdout", ""),
            stderr=result.get("stderr", ""),
            error=result.get("error"),
            execution_time_ms=elapsed_ms,
        )
    except Exception as e:
        elapsed_ms = int((time.monotonic() - start) * 1000)
        await update_execution(
            db, execution_id,
            status="error",
            error=str(e),
            execution_time_ms=elapsed_ms,
        )
```

#### `GET /executions/{id}` → 200

Read from SQLite instead of in-memory dict:

```python
@router.get("/executions/{execution_id}")
async def get_execution_status(execution_id: str) -> dict:
    """Poll execution status and results."""
    db = await get_db()
    record = await get_execution(db, execution_id)
    if record is None:
        raise HTTPException(status_code=404, detail="Execution not found")

    response: dict = {
        "execution_id": record["id"],
        "status": record["status"],
    }

    if record["status"] in ("completed", "error", "timeout"):
        response["result"] = record["result"]
        response["stdout"] = record["stdout"]
        response["stderr"] = record["stderr"]
        response["error"] = record["error"]
        response["execution_time_ms"] = record["execution_time_ms"]

    return response
```

#### `POST /executions/{id}/respond` — Keep as-is

The LLM pause/resume endpoint stays unchanged for now. It will be fully wired in Phase 6. For now it remains a stub that returns the mock behavior.

### 3. Execution History Endpoints

#### Agent API — `GET /executions` → 200

Agents can list their own executions (filtered by the profile in their Bearer token).

```python
@router.get("/executions")
async def list_agent_executions(
    request: Request,
    profile: ProfileAuth = Depends(require_profile),
    status: str | None = None,
    limit: int = 50,
    offset: int = 0,
) -> dict:
    """List executions for the authenticated profile."""
    db = await get_db()
    records = await list_executions(
        db, profile_id=profile.profile_id, status=status, limit=min(limit, 100), offset=offset
    )
    return {
        "executions": [
            {
                "execution_id": r["id"],
                "status": r["status"],
                "execution_time_ms": r["execution_time_ms"],
                "created_at": r["created_at"],
                "completed_at": r["completed_at"],
            }
            for r in records
        ]
    }
```

Note: The agent execution list returns **summary only** (no result/stdout/stderr). Use `GET /executions/{id}` for full details.

#### Admin API — `GET /api/admin/executions` → 200

Admin can list all executions with full filtering:

```python
@router.get("/executions", dependencies=[Depends(require_admin)])
async def admin_list_executions(
    profile_id: str | None = None,
    status: str | None = None,
    limit: int = 50,
    offset: int = 0,
) -> dict:
    """List all executions with optional filters."""
    db = await get_db()
    records = await list_executions(
        db, profile_id=profile_id, status=status, limit=min(limit, 100), offset=offset
    )
    return {
        "executions": [
            {
                "execution_id": r["id"],
                "profile_id": r["profile_id"],
                "status": r["status"],
                "result": r["result"],
                "stdout": r["stdout"],
                "stderr": r["stderr"],
                "error": r["error"],
                "execution_time_ms": r["execution_time_ms"],
                "created_at": r["created_at"],
                "completed_at": r["completed_at"],
            }
            for r in records
        ]
    }
```

#### Admin API — `GET /api/admin/executions/{id}` → 200

Admin can view any execution's full details, including the script:

```python
@router.get("/executions/{execution_id}", dependencies=[Depends(require_admin)])
async def admin_get_execution(execution_id: str) -> dict:
    """Get full execution details including script."""
    db = await get_db()
    # Fetch with script included
    cursor = await db.execute(
        """SELECT id, profile_id, script, status, result, stdout, stderr,
                  error, execution_time_ms, created_at, completed_at
           FROM executions WHERE id = ?""",
        (execution_id,),
    )
    row = await cursor.fetchone()
    if row is None:
        raise HTTPException(status_code=404, detail="Execution not found")

    return {
        "execution_id": row["id"],
        "profile_id": row["profile_id"],
        "script": row["script"],
        "status": row["status"],
        "result": json.loads(row["result"]) if row["result"] else None,
        "stdout": row["stdout"] or "",
        "stderr": row["stderr"] or "",
        "error": row["error"],
        "execution_time_ms": row["execution_time_ms"],
        "created_at": row["created_at"],
        "completed_at": row["completed_at"],
    }
```

### 4. Wire WorkerManager to App State — Update `src/airlock/app.py`

Move the WorkerManager to `app.state` so routes can access it without globals:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    await init_db()
    master_key = get_or_create_master_key(DATA_DIR)
    app.state.master_key = master_key

    worker_manager = None
    worker_enabled = os.environ.get("AIRLOCK_WORKER_ENABLED", "true").lower() == "true"

    if worker_enabled:
        worker_manager = WorkerManager()
        try:
            await worker_manager.start()
            logger.info("Worker container started")
        except Exception as e:
            logger.warning(f"Worker container failed to start: {e}")
            worker_manager = None

    app.state.worker_manager = worker_manager  # ← store on app.state

    yield

    if worker_manager:
        await worker_manager.stop()
    await close_db()
```

Remove the `set_worker_manager()` call from lifespan.

### 5. Graceful Worker Unavailability

If the worker container isn't running (Docker not installed, build failed, etc.), `POST /execute` should fail gracefully:

```python
# In the execute endpoint, before dispatching:
worker = raw_request.app.state.worker_manager
if worker is None or not worker.is_running():
    # Clean up the pending execution record
    await update_execution(db, execution_id, status="error", error="Worker not available")
    raise HTTPException(
        status_code=503,
        detail="Execution engine is not available. Docker worker container is not running.",
    )
```

### 6. Update Pydantic Models — `src/airlock/models.py`

The `ExecutionRequest` model was updated in Spec 3.1 (Bearer auth + hash). No changes needed to the request model.

Add response models for execution history:

```python
class ExecutionSummary(BaseModel):
    """Execution summary for list endpoints."""
    execution_id: str
    status: str
    execution_time_ms: int | None = None
    created_at: str
    completed_at: str | None = None

class ExecutionDetail(BaseModel):
    """Full execution detail."""
    execution_id: str
    profile_id: str
    status: str
    result: Any | None = None
    stdout: str = ""
    stderr: str = ""
    error: str | None = None
    execution_time_ms: int | None = None
    created_at: str
    completed_at: str | None = None

class AdminExecutionDetail(ExecutionDetail):
    """Admin execution detail (includes script)."""
    script: str
```

---

## What Does NOT Change

- Worker container (`Dockerfile.worker`, `worker/server.py`, `worker/sdk.py`) — unchanged
- Credential system (`crypto.py`, `services/credentials.py`) — unchanged
- Profile system (`services/profiles.py`, `auth.py`) — unchanged
- `GET /skill.md` — unchanged (dynamic content in Phase 7)
- Web UI — shells exist, wiring is a UI concern
- LLM pause/resume — stub stays, full implementation in Phase 6
- Output sanitization — Phase 5

## What This Enables

After this spec:
- **Agents can execute real Python code** — not mocks, actual Docker-isolated execution
- **Credentials are injected** — `settings.get("API_KEY")` returns real values inside the worker
- **Results survive restarts** — execution history persisted in SQLite
- **`poll_url` is returned** — agents get a full URL for load-balancer-safe polling
- **Execution history is queryable** — agents see their own, admins see everything
- **Graceful degradation** — if Docker isn't available, clear error instead of crash

---

## Tests

### Unit Tests — `tests/test_execution_engine.py` (new file)

#### Execution Service (SQLite persistence)

- `create_execution` → returns `exec_...` ID, record exists in DB with status `pending`
- `update_execution` with `completed` → status updated, `completed_at` set
- `update_execution` with `error` → error message stored, `completed_at` set
- `update_execution` with `running` → status updated, `completed_at` remains NULL
- `get_execution` → returns full record
- `get_execution` with nonexistent ID → returns None
- `list_executions` → returns records newest-first
- `list_executions` with `profile_id` filter → only matching records
- `list_executions` with `status` filter → only matching records
- `list_executions` with `limit`/`offset` → pagination works
- `list_executions` on empty DB → empty list

#### Execute Endpoint (mock worker)

For tests that don't need Docker, mock the WorkerManager:

- `POST /execute` with valid auth + HMAC → 202 with `execution_id`, `poll_url`, `status: "pending"`
- `POST /execute` → `poll_url` contains full URL (starts with `http`)
- `POST /execute` → execution record created in SQLite
- `POST /execute` without auth → 401
- `POST /execute` with wrong HMAC → 403
- `POST /execute` when worker unavailable → 503 with clear error
- After execution completes → `GET /executions/{id}` returns result from worker

#### Execute Endpoint (real Docker)

These tests require Docker and are marked to skip without it:

- `POST /execute` with simple script → poll until completed → result matches
- `POST /execute` with `settings.get()` → credential value accessible in worker
- `POST /execute` with `settings.keys()` → lists credential names
- `POST /execute` with `set_result()` → result captured
- `POST /execute` with stdout → stdout captured
- `POST /execute` exceeding timeout → status is `timeout`
- `POST /execute` with exception → status is `error`, error message captured

#### Execution History

- `GET /executions` (agent) → returns summary list for authenticated profile only
- `GET /executions` (agent) with status filter → filtered results
- `GET /executions` (agent) → does NOT include other profiles' executions
- `GET /api/admin/executions` → returns all executions with full details
- `GET /api/admin/executions` with profile_id filter → filtered
- `GET /api/admin/executions/{id}` → includes script
- `GET /api/admin/executions/{id}` nonexistent → 404
- All admin endpoints require auth → 401 without token

#### poll_url

- Returned `poll_url` is a valid full URL
- `GET {poll_url}` returns the execution status
- `poll_url` matches `/executions/{execution_id}` pattern

### Existing Tests

- All existing tests from Specs 1-3 must pass
- `test_agent_api.py` mock execution tests may need updating (they use the old in-memory approach)
- `test_docker_execution.py` tests the worker directly — these should still pass (worker unchanged)

---

## Implementation Order

1. `src/airlock/services/executions.py` — execution service (CRUD + SQLite)
2. Update `src/airlock/models.py` — add execution history response models
3. Update `src/airlock/api/agent.py` — replace mock with real worker dispatch + SQLite + poll_url + `GET /executions` list
4. Update `src/airlock/api/admin.py` — add execution history endpoints
5. Update `src/airlock/app.py` — move WorkerManager to app.state, remove set_worker_manager
6. `tests/test_execution_engine.py` — all tests
7. Update `tests/test_agent_api.py` — fix mock tests for new execution flow
8. Verify all existing tests pass

---

## Acceptance Criteria

- [ ] `POST /execute` dispatches to Docker worker (not in-memory `exec()`)
- [ ] `POST /execute` returns `poll_url` as full URL
- [ ] Execution records persist in SQLite `executions` table
- [ ] `GET /executions/{id}` reads from SQLite
- [ ] Credentials resolved and injected into worker via `settings.get()`
- [ ] Execution timing tracked (`execution_time_ms`)
- [ ] `completed_at` set when execution finishes
- [ ] `GET /executions` (agent, authenticated) → own profile's executions only
- [ ] `GET /api/admin/executions` → all executions with full details
- [ ] `GET /api/admin/executions/{id}` → includes script
- [ ] Worker unavailable → 503 with clear error (not crash)
- [ ] `_executions` in-memory dict removed
- [ ] `set_worker_manager()` removed, worker on `app.state`
- [ ] All new tests pass
- [ ] All existing tests updated and pass

---

## Notes for CC

- The `executions` table already exists in `db.py` — don't recreate it.
- `WorkerManager` is in `src/airlock/worker_manager.py` — it has `start()`, `stop()`, `execute(script, settings, timeout)`, and `is_running()`. The `execute()` method returns a dict with `status`, `result`, `stdout`, `stderr`, `error`.
- The worker container runs on port 8001 internally. WorkerManager handles all Docker lifecycle (build, start, health check, stop).
- Remove the `_executions` dict and `_run_execution` function from `agent.py`. Remove `set_worker_manager()` — the worker is on `app.state.worker_manager`.
- For mock tests (no Docker), mock `app.state.worker_manager` with a fake that returns `{"status": "completed", "result": ..., "stdout": "", "stderr": ""}`.
- The `result` column in SQLite stores JSON text — use `json.dumps()` when writing, `json.loads()` when reading.
- `poll_url` should use `request.base_url` to build the full URL, not hardcode a host.
- Keep `POST /executions/{id}/respond` as-is (LLM pause/resume is Phase 6).
- `GET /executions` (agent) requires Bearer auth via `require_profile` — this scopes results to that profile. Don't expose other profiles' executions.
- Admin endpoints under `/api/admin/executions` require session token via `require_admin`.
- Execution IDs: `exec_` + 16 hex chars (shorter than full UUID, still unique enough).
- The `AIRLOCK_WORKER_ENABLED` env var controls whether the worker starts. Default is `true`.
