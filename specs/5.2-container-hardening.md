# Spec 5.2: Container Hardening (Phase 5 → v0.5.0)

## Goal

Harden the Docker worker container with resource limits, read-only filesystem, and security options. Workers should be constrained so a malicious or buggy script can't consume all host resources or escape the sandbox.

**Before:** Worker runs with default Docker settings — no memory cap, no CPU limit, writable filesystem, default security options.
**After:** Worker runs with enforced resource limits, read-only root filesystem (except `/tmp`), `--no-new-privileges`, and configurable limits.

## Prerequisites

- Spec 1.2 (Docker Execution) ✅ — WorkerManager + Dockerfile.worker exist
- Spec 4.1 (Execution Engine) ✅ — worker dispatch is wired

---

## Architecture

```
WorkerManager.start()
        │
        ▼
docker run \
  --memory=512m \
  --memory-swap=512m \
  --cpus=1.0 \
  --read-only \
  --tmpfs /tmp:rw,noexec,nosuid,size=100m \
  --security-opt=no-new-privileges:true \
  --pids-limit=100 \
  --name airlock-worker \
  airlock-worker
```

---

## Tasks

### 1. Update WorkerManager — `src/airlock/worker_manager.py`

Add resource limits and security options to the `docker run` command:

```python
# Default resource limits (configurable via environment)
DEFAULT_MEMORY_LIMIT = os.environ.get("AIRLOCK_WORKER_MEMORY", "512m")
DEFAULT_CPU_LIMIT = os.environ.get("AIRLOCK_WORKER_CPUS", "1.0")
DEFAULT_TMP_SIZE = os.environ.get("AIRLOCK_WORKER_TMP_SIZE", "100m")
DEFAULT_PIDS_LIMIT = os.environ.get("AIRLOCK_WORKER_PIDS_LIMIT", "100")

async def start(self) -> None:
    # ... existing cleanup and build ...

    # Run the container with hardened settings
    rc, out, err = await _run_docker(
        "run", "-d",
        "--name", CONTAINER_NAME,
        "--network", NETWORK_NAME,
        "-p", f"{WORKER_PORT}:{WORKER_PORT}",
        # Resource limits
        f"--memory={DEFAULT_MEMORY_LIMIT}",
        f"--memory-swap={DEFAULT_MEMORY_LIMIT}",  # No swap
        f"--cpus={DEFAULT_CPU_LIMIT}",
        f"--pids-limit={DEFAULT_PIDS_LIMIT}",
        # Filesystem
        "--read-only",
        f"--tmpfs=/tmp:rw,noexec,nosuid,size={DEFAULT_TMP_SIZE}",
        # Security
        "--security-opt=no-new-privileges:true",
        IMAGE_NAME,
    )
```

### 2. Update Dockerfile.worker

Ensure the worker is fully compatible with read-only filesystem:

```dockerfile
FROM python:3.12-slim

# Create non-root user
RUN useradd --create-home worker

# Install dependencies
RUN pip install --no-cache-dir fastapi uvicorn

# Copy worker code
COPY src/airlock/worker/ /app/worker/

WORKDIR /app

# Ensure no writable directories are needed outside /tmp
# Python bytecode cache goes to /tmp
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

USER worker

EXPOSE 8001

CMD ["uvicorn", "worker.server:app", "--host", "0.0.0.0", "--port", "8001"]
```

### 3. Handle Resource Limit Errors

Update the worker `server.py` to handle OOM and resource limit kills gracefully. When a script is killed by the OOM killer or PID limit, the worker should report a clear error:

The thread-based execution in `server.py` already captures exceptions. OOM kills will terminate the container, so the `WorkerManager` needs to handle the case where the HTTP connection drops:

```python
# In WorkerManager.execute():
async def execute(
    self, script: str, settings: dict[str, str] | None = None, timeout: int = 60
) -> dict[str, Any]:
    """Send a script to the worker for execution."""
    http_timeout = timeout + 10
    try:
        resp = await self._client.post(
            "/run",
            json={"script": script, "settings": settings or {}, "timeout": timeout},
            timeout=http_timeout,
        )
        resp.raise_for_status()
        return resp.json()
    except httpx.RemoteProtocolError:
        # Worker container was likely killed (OOM, etc.)
        return {
            "status": "error",
            "result": None,
            "stdout": "",
            "stderr": "",
            "error": "Worker process was killed — likely exceeded memory limit",
        }
    except httpx.ConnectError:
        return {
            "status": "error",
            "result": None,
            "stdout": "",
            "stderr": "",
            "error": "Worker container is not responding",
        }
```

### 4. Auto-Restart Worker After Crash

If the worker container dies (OOM kill), the WorkerManager should be able to restart it:

```python
async def _ensure_running(self) -> bool:
    """Check if worker is alive, restart if dead."""
    try:
        resp = await self._client.get("/health", timeout=2)
        return resp.status_code == 200
    except (httpx.ConnectError, httpx.RemoteProtocolError):
        pass

    # Container died — try to restart
    logger.warning("Worker container died, attempting restart...")
    await _run_docker("rm", "-f", CONTAINER_NAME)
    try:
        await self.start()
        return True
    except Exception as e:
        logger.error(f"Failed to restart worker: {e}")
        return False
```

Call `_ensure_running()` before dispatching each execution.

---

## What Does NOT Change

- Worker code (`server.py`, `sdk.py`) — unchanged
- API endpoints — unchanged
- Credential/profile system — unchanged
- Sanitization (5.1) — independent

## What This Enables

After this spec:
- Scripts can't consume unlimited memory (capped at 512MB by default)
- Scripts can't fork-bomb (PID limit)
- Scripts can't write to the filesystem (read-only, except /tmp)
- Scripts can't escalate privileges
- Worker auto-recovers after OOM kills
- Resource limits are configurable via environment variables

---

## Tests

### Unit Tests — `tests/test_container_hardening.py` (new file)

These tests require Docker and should be marked to skip without it.

#### Resource Limits
- Worker starts with `--memory` flag → `docker inspect` confirms limit set
- Worker starts with `--cpus` flag → `docker inspect` confirms CPU limit
- Worker starts with `--pids-limit` → `docker inspect` confirms PID limit
- Worker starts with `--read-only` → `docker inspect` confirms read-only rootfs
- Script writing to `/app/` → permission denied error
- Script writing to `/tmp/` → succeeds (tmpfs mounted rw)

#### OOM Handling
- Script allocating large memory (> limit) → execution returns error status
- Worker auto-restarts after OOM kill → next execution succeeds

#### Environment Configuration
- Custom `AIRLOCK_WORKER_MEMORY` → worker uses custom limit
- Custom `AIRLOCK_WORKER_CPUS` → worker uses custom CPU limit
- Default values used when env vars not set

#### Security Options
- Worker runs with `--no-new-privileges` → verified via docker inspect
- Worker runs as non-root user → verified via `whoami` in worker

### Existing Tests
- All existing tests must pass (Docker tests already marked skip-if-unavailable)

---

## Implementation Order

1. Update `src/airlock/worker_manager.py` — add resource limits, error handling, auto-restart
2. Update `Dockerfile.worker` — add PYTHONDONTWRITEBYTECODE, ensure read-only compatibility
3. `tests/test_container_hardening.py` — all tests
4. Verify all existing tests pass

---

## Acceptance Criteria

- [ ] `docker inspect airlock-worker` shows memory limit set
- [ ] `docker inspect airlock-worker` shows CPU limit set
- [ ] `docker inspect airlock-worker` shows PID limit set
- [ ] `docker inspect airlock-worker` shows read-only rootfs
- [ ] `docker inspect airlock-worker` shows `no-new-privileges`
- [ ] Scripts can write to `/tmp` but not to `/app/` or `/`
- [ ] OOM kill returns clear error (not hang or crash)
- [ ] Worker auto-restarts after container death
- [ ] Resource limits configurable via `AIRLOCK_WORKER_*` env vars
- [ ] All new tests pass
- [ ] All existing tests pass

---

## Notes for CC

- The `docker run` command in `WorkerManager.start()` currently uses minimal flags. Add all resource/security flags there.
- `--memory-swap` set equal to `--memory` means no swap — process gets killed if it exceeds the limit rather than swapping.
- `--tmpfs` mounts a writable tmpfs at `/tmp`. `noexec` prevents executing binaries from `/tmp`. `nosuid` prevents setuid. `size=100m` caps tmpfs size.
- `--pids-limit=100` prevents fork bombs. Python scripts typically use 1-2 processes, so 100 is generous.
- The `_ensure_running()` method should be called at the start of `execute()`, not in the background task. This way, if the worker died, it gets restarted before the execution attempt.
- For OOM detection: when the container is killed, `httpx` will get a `RemoteProtocolError` (connection closed mid-response) or `ConnectError` (container gone). Catch both.
- Environment variables for limits follow the pattern `AIRLOCK_WORKER_*`. Read them once at module level or in `__init__`.
